{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5693788",
   "metadata": {},
   "source": [
    "# Text Processing Basics for Fake News Detection\n",
    "\n",
    "This notebook covers the fundamental text processing concepts required for our fake news detection project:\n",
    "\n",
    "1. **Tokenization**: Breaking text into individual words or tokens\n",
    "2. **Stopword Removal**: Removing common words that don't carry much meaning\n",
    "3. **Vectorization**: Converting text into numerical format for machine learning\n",
    "\n",
    "Let's start by importing the necessary libraries and loading our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK specific imports\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Scikit-learn imports for vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our sample data\n",
    "df = pd.read_csv('../data/sample_data.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf3b6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"\\n0: Real News\")\n",
    "print(\"1: Fake News\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df, x='label')\n",
    "plt.title('Distribution of Real vs Fake News')\n",
    "plt.xlabel('Label (0=Real, 1=Fake)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438bb81",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "**Tokenization** is the process of breaking down text into individual words, phrases, symbols, or other meaningful elements called tokens.\n",
    "\n",
    "### Types of Tokenization:\n",
    "- **Word Tokenization**: Splitting text into individual words\n",
    "- **Sentence Tokenization**: Splitting text into sentences\n",
    "- **Subword Tokenization**: Breaking words into smaller units\n",
    "\n",
    "### Why is Tokenization Important?\n",
    "- Computers can't understand raw text\n",
    "- It's the first step in text preprocessing\n",
    "- Helps in feature extraction and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bace096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text for demonstration\n",
    "sample_text = df.iloc[0]['text']\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f3c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Simple tokenization using split()\n",
    "simple_tokens = sample_text.split()\n",
    "print(\"Simple tokenization using split():\")\n",
    "print(simple_tokens)\n",
    "print(f\"Number of tokens: {len(simple_tokens)}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 NLTK word tokenization (more sophisticated)\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "print(\"NLTK word tokenization:\")\n",
    "print(nltk_tokens)\n",
    "print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Regular expression tokenization\n",
    "# This pattern matches sequences of word characters (letters, digits, underscore)\n",
    "regex_tokens = re.findall(r'\\b\\w+\\b', sample_text.lower())\n",
    "print(\"Regular expression tokenization:\")\n",
    "print(regex_tokens)\n",
    "print(f\"Number of tokens: {len(regex_tokens)}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd151858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Sentence tokenization\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print(\"Sentence tokenization:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "print(f\"\\nNumber of sentences: {len(sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e16f01",
   "metadata": {},
   "source": [
    "### Tokenization Function for Our Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, method='nltk'):\n",
    "    \"\"\"\n",
    "    Tokenize text using different methods\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to tokenize\n",
    "        method (str): Tokenization method ('simple', 'nltk', 'regex')\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tokens\n",
    "    \"\"\"\n",
    "    if method == 'simple':\n",
    "        return text.split()\n",
    "    elif method == 'nltk':\n",
    "        return word_tokenize(text.lower())\n",
    "    elif method == 'regex':\n",
    "        return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'simple', 'nltk', or 'regex'\")\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This is a test! Can you tokenize this?\"\n",
    "print(\"Original text:\", test_text)\n",
    "print(\"Simple:\", tokenize_text(test_text, 'simple'))\n",
    "print(\"NLTK:\", tokenize_text(test_text, 'nltk'))\n",
    "print(\"Regex:\", tokenize_text(test_text, 'regex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c1a09",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal\n",
    "\n",
    "**Stopwords** are common words that typically don't carry much meaning and are often filtered out during text processing.\n",
    "\n",
    "### Examples of stopwords:\n",
    "- Articles: a, an, the\n",
    "- Prepositions: in, on, at, by\n",
    "- Pronouns: I, you, he, she, it\n",
    "- Common verbs: is, are, was, were\n",
    "\n",
    "### Why remove stopwords?\n",
    "- Reduces noise in the data\n",
    "- Decreases computational complexity\n",
    "- Focuses on meaningful words\n",
    "- Improves model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32321ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Number of stopwords in NLTK: {len(stop_words)}\")\n",
    "print(\"\\nFirst 20 stopwords:\")\n",
    "print(list(stop_words)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad2951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Remove stopwords from our sample text\n",
    "tokens = word_tokenize(sample_text.lower())\n",
    "print(\"Original tokens:\")\n",
    "print(tokens)\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "print(\"Tokens after stopword removal:\")\n",
    "print(filtered_tokens)\n",
    "print(f\"Number of tokens: {len(filtered_tokens)}\")\n",
    "print(f\"Reduction: {len(tokens) - len(filtered_tokens)} tokens removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d656878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom stopword removal function\n",
    "def remove_stopwords(tokens, custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from a list of tokens\n",
    "    \n",
    "    Args:\n",
    "        tokens (list): List of tokens\n",
    "        custom_stopwords (set): Custom set of stopwords (optional)\n",
    "    \n",
    "    Returns:\n",
    "        list: Filtered tokens without stopwords\n",
    "    \"\"\"\n",
    "    if custom_stopwords is None:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:\n",
    "        stop_words = custom_stopwords\n",
    "    \n",
    "    # Filter out stopwords and non-alphabetic tokens\n",
    "    filtered_tokens = [token for token in tokens \n",
    "                      if token.lower() not in stop_words and token.isalpha()]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Test the function\n",
    "test_tokens = ['this', 'is', 'a', 'great', 'example', 'of', 'text', 'processing']\n",
    "print(\"Original tokens:\", test_tokens)\n",
    "print(\"After stopword removal:\", remove_stopwords(test_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the impact of stopword removal on our dataset\n",
    "def analyze_stopword_impact(text):\n",
    "    \"\"\"\n",
    "    Analyze the impact of stopword removal on text\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    original_count = len(tokens)\n",
    "    \n",
    "    # Remove stopwords and non-alphabetic tokens\n",
    "    filtered_tokens = remove_stopwords(tokens)\n",
    "    filtered_count = len(filtered_tokens)\n",
    "    \n",
    "    reduction_percentage = ((original_count - filtered_count) / original_count) * 100\n",
    "    \n",
    "    return {\n",
    "        'original_count': original_count,\n",
    "        'filtered_count': filtered_count,\n",
    "        'reduction_percentage': reduction_percentage\n",
    "    }\n",
    "\n",
    "# Analyze impact on a few sample texts\n",
    "for i in range(3):\n",
    "    text = df.iloc[i]['text']\n",
    "    label = 'Real' if df.iloc[i]['label'] == 0 else 'Fake'\n",
    "    impact = analyze_stopword_impact(text)\n",
    "    \n",
    "    print(f\"Text {i+1} ({label} News):\")\n",
    "    print(f\"  Original tokens: {impact['original_count']}\")\n",
    "    print(f\"  After filtering: {impact['filtered_count']}\")\n",
    "    print(f\"  Reduction: {impact['reduction_percentage']:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6159660",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Pipeline\n",
    "\n",
    "Before vectorization, let's create a comprehensive text preprocessing pipeline that includes:\n",
    "- Text cleaning\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, remove_stopwords_flag=True, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        remove_stopwords_flag (bool): Whether to remove stopwords\n",
    "        use_stemming (bool): Whether to apply stemming\n",
    "        use_lemmatization (bool): Whether to apply lemmatization\n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords_flag:\n",
    "        tokens = remove_stopwords(tokens)\n",
    "    \n",
    "    # Apply stemming or lemmatization\n",
    "    if use_stemming:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif use_lemmatization:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the preprocessing pipeline\n",
    "sample_text = df.iloc[1]['text']  # A fake news example\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "print(\"After preprocessing:\")\n",
    "processed_text = preprocess_text(sample_text)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151a6ad",
   "metadata": {},
   "source": [
    "## 4. Vectorization\n",
    "\n",
    "**Vectorization** is the process of converting text into numerical format that machine learning algorithms can understand.\n",
    "\n",
    "### Common Vectorization Techniques:\n",
    "\n",
    "1. **Bag of Words (BoW)**: Represents text as a collection of words, ignoring grammar and word order\n",
    "2. **Term Frequency-Inverse Document Frequency (TF-IDF)**: Reflects how important a word is to a document in a collection of documents\n",
    "3. **N-grams**: Sequences of n consecutive words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8fe2ec",
   "metadata": {},
   "source": [
    "### 4.1 Count Vectorization (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b62d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a small sample for demonstration\n",
    "sample_texts = [\n",
    "    \"This is a real news article about science\",\n",
    "    \"This fake news spreads misinformation\",\n",
    "    \"Science article provides accurate information\",\n",
    "    \"Fake article spreads false claims\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Convert to lowercase\n",
    "    stop_words='english',     # Remove English stopwords\n",
    "    max_features=1000,        # Limit vocabulary size\n",
    "    ngram_range=(1, 1)        # Use unigrams only\n",
    ")\n",
    "\n",
    "# Fit and transform the sample texts\n",
    "count_matrix = count_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(\"Vocabulary (feature names):\")\n",
    "print(feature_names)\n",
    "print(f\"\\nVocabulary size: {len(feature_names)}\")\n",
    "\n",
    "# Convert to dense array for better visualization\n",
    "count_dense = count_matrix.toarray()\n",
    "print(\"\\nCount matrix shape:\", count_dense.shape)\n",
    "print(\"\\nCount matrix:\")\n",
    "count_df = pd.DataFrame(count_dense, columns=feature_names)\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the count matrix\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(count_df, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Count Vectorization Matrix')\n",
    "plt.xlabel('Words (Features)')\n",
    "plt.ylabel('Documents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185a307c",
   "metadata": {},
   "source": [
    "### 4.2 TF-IDF Vectorization\n",
    "\n",
    "**TF-IDF** combines two metrics:\n",
    "- **TF (Term Frequency)**: How frequently a term appears in a document\n",
    "- **IDF (Inverse Document Frequency)**: How rare or common a term is across all documents\n",
    "\n",
    "**Formula**: TF-IDF = TF Ã— IDF\n",
    "\n",
    "- TF = (Number of times term appears in document) / (Total number of terms in document)\n",
    "- IDF = log(Total number of documents / Number of documents containing the term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,           # Convert to lowercase\n",
    "    stop_words='english',     # Remove English stopwords\n",
    "    max_features=1000,        # Limit vocabulary size\n",
    "    ngram_range=(1, 2),       # Use unigrams and bigrams\n",
    "    min_df=1,                 # Minimum document frequency\n",
    "    max_df=0.95               # Maximum document frequency\n",
    ")\n",
    "\n",
    "# Fit and transform the sample texts\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_texts)\n",
    "\n",
    "# Get feature names\n",
    "tfidf_features = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"TF-IDF Features:\")\n",
    "print(tfidf_features)\n",
    "print(f\"\\nFeature count: {len(tfidf_features)}\")\n",
    "\n",
    "# Convert to dense array\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "print(\"\\nTF-IDF matrix shape:\", tfidf_dense.shape)\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_dense, columns=tfidf_features)\n",
    "print(\"\\nTF-IDF matrix (showing non-zero values):\")\n",
    "# Display only columns with non-zero values\n",
    "non_zero_cols = tfidf_df.columns[tfidf_df.sum() > 0]\n",
    "print(tfidf_df[non_zero_cols].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize TF-IDF scores\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(tfidf_df[non_zero_cols], annot=True, cmap='viridis', fmt='.3f')\n",
    "plt.title('TF-IDF Vectorization Matrix')\n",
    "plt.xlabel('Features (Words/N-grams)')\n",
    "plt.ylabel('Documents')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916c825",
   "metadata": {},
   "source": [
    "### 4.3 Comparing Count vs TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vectorization_methods(texts):\n",
    "    \"\"\"\n",
    "    Compare Count and TF-IDF vectorization methods\n",
    "    \"\"\"\n",
    "    # Count Vectorization\n",
    "    count_vec = CountVectorizer(stop_words='english', max_features=50)\n",
    "    count_matrix = count_vec.fit_transform(texts)\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', max_features=50)\n",
    "    tfidf_matrix = tfidf_vec.fit_transform(texts)\n",
    "    \n",
    "    print(\"Count Vectorization:\")\n",
    "    print(f\"  Matrix shape: {count_matrix.shape}\")\n",
    "    print(f\"  Non-zero elements: {count_matrix.nnz}\")\n",
    "    print(f\"  Sparsity: {(1 - count_matrix.nnz / (count_matrix.shape[0] * count_matrix.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nTF-IDF Vectorization:\")\n",
    "    print(f\"  Matrix shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"  Non-zero elements: {tfidf_matrix.nnz}\")\n",
    "    print(f\"  Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "    \n",
    "    return count_matrix, tfidf_matrix, count_vec, tfidf_vec\n",
    "\n",
    "# Apply preprocessing to our dataset\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Compare methods on our dataset\n",
    "count_mat, tfidf_mat, count_vec, tfidf_vec = compare_vectorization_methods(df['processed_text'])\n",
    "\n",
    "print(\"\\nFirst few processed texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"Text {i+1}: {df['processed_text'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460f200",
   "metadata": {},
   "source": [
    "### 4.4 N-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze different n-gram combinations\n",
    "def analyze_ngrams(texts, ngram_range=(1, 1), max_features=20):\n",
    "    \"\"\"\n",
    "    Analyze n-grams in the text corpus\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=max_features,\n",
    "        stop_words='english'\n",
    "    )\n",
    "    \n",
    "    matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Calculate mean TF-IDF scores\n",
    "    mean_scores = matrix.mean(axis=0).A1\n",
    "    feature_scores = list(zip(feature_names, mean_scores))\n",
    "    feature_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return feature_scores\n",
    "\n",
    "# Analyze unigrams\n",
    "print(\"Top 15 Unigrams (1-grams):\")\n",
    "unigrams = analyze_ngrams(df['processed_text'], ngram_range=(1, 1), max_features=15)\n",
    "for feature, score in unigrams:\n",
    "    print(f\"  {feature}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 Bigrams (2-grams):\")\n",
    "bigrams = analyze_ngrams(df['processed_text'], ngram_range=(2, 2), max_features=15)\n",
    "for feature, score in bigrams:\n",
    "    print(f\"  {feature}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 15 Trigrams (3-grams):\")\n",
    "trigrams = analyze_ngrams(df['processed_text'], ngram_range=(3, 3), max_features=15)\n",
    "for feature, score in trigrams:\n",
    "    print(f\"  {feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f48bf",
   "metadata": {},
   "source": [
    "### 4.5 Feature Engineering for Fake News Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive vectorization pipeline for fake news detection\n",
    "def create_features(texts, vectorizer_type='tfidf', ngram_range=(1, 2), max_features=5000):\n",
    "    \"\"\"\n",
    "    Create feature matrix for fake news detection\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of preprocessed texts\n",
    "        vectorizer_type (str): 'count' or 'tfidf'\n",
    "        ngram_range (tuple): N-gram range\n",
    "        max_features (int): Maximum number of features\n",
    "    \n",
    "    Returns:\n",
    "        sparse matrix: Feature matrix\n",
    "        vectorizer: Fitted vectorizer\n",
    "    \"\"\"\n",
    "    if vectorizer_type == 'count':\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "            max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
    "        )\n",
    "    else:  # tfidf\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features,\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.8,\n",
    "            sublinear_tf=True  # Apply sublinear tf scaling\n",
    "        )\n",
    "    \n",
    "    feature_matrix = vectorizer.fit_transform(texts)\n",
    "    return feature_matrix, vectorizer\n",
    "\n",
    "# Create features for our dataset\n",
    "X, vectorizer = create_features(df['processed_text'], vectorizer_type='tfidf')\n",
    "y = df['label']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of samples: {X.shape[0]}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Sparsity: {(1 - X.nnz / (X.shape[0] * X.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Get top features\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"\\nTop 20 features: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d0420",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together: Complete Text Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305687cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \"\"\"\n",
    "    Complete text processing pipeline for fake news detection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vectorizer_type='tfidf', ngram_range=(1, 2), max_features=5000):\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "        self.vectorizer = None\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def preprocess_text(self, text, remove_stopwords=True, use_lemmatization=True):\n",
    "        \"\"\"Preprocess individual text\"\"\"\n",
    "        # Clean text\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        if remove_stopwords:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "        \n",
    "        # Lemmatization\n",
    "        if use_lemmatization:\n",
    "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Fit vectorizer and transform texts\"\"\"\n",
    "        # Preprocess all texts\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Initialize vectorizer\n",
    "        if self.vectorizer_type == 'count':\n",
    "            self.vectorizer = CountVectorizer(\n",
    "                ngram_range=self.ngram_range,\n",
    "                max_features=self.max_features,\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.8\n",
    "            )\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(\n",
    "                ngram_range=self.ngram_range,\n",
    "                max_features=self.max_features,\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.8,\n",
    "                sublinear_tf=True\n",
    "            )\n",
    "        \n",
    "        # Fit and transform\n",
    "        feature_matrix = self.vectorizer.fit_transform(processed_texts)\n",
    "        return feature_matrix\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Transform new texts using fitted vectorizer\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            raise ValueError(\"Vectorizer not fitted. Call fit_transform first.\")\n",
    "        \n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        return self.vectorizer.transform(processed_texts)\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Get feature names from vectorizer\"\"\"\n",
    "        if self.vectorizer is None:\n",
    "            raise ValueError(\"Vectorizer not fitted. Call fit_transform first.\")\n",
    "        return self.vectorizer.get_feature_names_out()\n",
    "\n",
    "# Test the complete pipeline\n",
    "processor = TextProcessor(vectorizer_type='tfidf', ngram_range=(1, 2), max_features=100)\n",
    "X_processed = processor.fit_transform(df['text'])\n",
    "\n",
    "print(f\"Processed feature matrix shape: {X_processed.shape}\")\n",
    "print(f\"Feature names (first 10): {processor.get_feature_names()[:10]}\")\n",
    "\n",
    "# Test on new text\n",
    "new_text = [\"This is breaking news about a scientific discovery that will change everything!\"]\n",
    "new_features = processor.transform(new_text)\n",
    "print(f\"\\nNew text feature shape: {new_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cfaf1",
   "metadata": {},
   "source": [
    "## 6. Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe21527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze vocabulary differences between real and fake news\n",
    "def analyze_class_vocabulary(df, processor):\n",
    "    \"\"\"\n",
    "    Analyze vocabulary differences between real and fake news\n",
    "    \"\"\"\n",
    "    real_news = df[df['label'] == 0]['text'].tolist()\n",
    "    fake_news = df[df['label'] == 1]['text'].tolist()\n",
    "    \n",
    "    # Process separately\n",
    "    real_processed = [processor.preprocess_text(text) for text in real_news]\n",
    "    fake_processed = [processor.preprocess_text(text) for text in fake_news]\n",
    "    \n",
    "    # Get word frequencies\n",
    "    real_words = ' '.join(real_processed).split()\n",
    "    fake_words = ' '.join(fake_processed).split()\n",
    "    \n",
    "    real_freq = Counter(real_words)\n",
    "    fake_freq = Counter(fake_words)\n",
    "    \n",
    "    print(\"Top 10 words in REAL news:\")\n",
    "    for word, count in real_freq.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    print(\"\\nTop 10 words in FAKE news:\")\n",
    "    for word, count in fake_freq.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "    \n",
    "    return real_freq, fake_freq\n",
    "\n",
    "real_freq, fake_freq = analyze_class_vocabulary(df, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word clouds for real vs fake news\n",
    "def create_wordclouds(real_freq, fake_freq):\n",
    "    \"\"\"\n",
    "    Create word clouds for real and fake news\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Real news word cloud\n",
    "    wordcloud_real = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(real_freq)\n",
    "    ax1.imshow(wordcloud_real, interpolation='bilinear')\n",
    "    ax1.set_title('Real News Word Cloud', fontsize=16)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Fake news word cloud\n",
    "    wordcloud_fake = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(fake_freq)\n",
    "    ax2.imshow(wordcloud_fake, interpolation='bilinear')\n",
    "    ax2.set_title('Fake News Word Cloud', fontsize=16)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_wordclouds(real_freq, fake_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c1625",
   "metadata": {},
   "source": [
    "## 7. Summary and Key Takeaways\n",
    "\n",
    "### Text Processing Pipeline Summary:\n",
    "\n",
    "1. **Text Cleaning**: Remove special characters, normalize case\n",
    "2. **Tokenization**: Break text into individual words\n",
    "3. **Stopword Removal**: Filter out common, non-informative words\n",
    "4. **Normalization**: Apply stemming or lemmatization\n",
    "5. **Vectorization**: Convert text to numerical features\n",
    "\n",
    "### Key Concepts Learned:\n",
    "\n",
    "- **Tokenization**: Essential first step in text processing\n",
    "- **Stopword Removal**: Reduces noise and improves model focus\n",
    "- **Count Vectorization**: Simple bag-of-words approach\n",
    "- **TF-IDF**: Weights words by importance across the corpus\n",
    "- **N-grams**: Capture word sequences and context\n",
    "\n",
    "### Best Practices for Fake News Detection:\n",
    "\n",
    "1. **Use TF-IDF over Count Vectorization**: Better for distinguishing important terms\n",
    "2. **Include Bigrams**: Capture phrases like \"breaking news\" or \"doctors hate\"\n",
    "3. **Set Appropriate min_df and max_df**: Filter very rare and very common terms\n",
    "4. **Preprocess Consistently**: Same pipeline for training and inference\n",
    "5. **Consider Domain-Specific Stopwords**: Add news-specific common words\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Model Training**: Use these features with ML algorithms (Naive Bayes, SVM, Random Forest)\n",
    "2. **Feature Engineering**: Add metadata features (text length, punctuation, etc.)\n",
    "3. **Advanced Techniques**: Explore word embeddings (Word2Vec, GloVe) or transformer models\n",
    "4. **Evaluation**: Test model performance and analyze feature importance"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
